---
---


@inproceedings{brunzema2022controller,
  title       = {On Controller Tuning with Time-Varying {Bayesian} Optimization},
  author      = {Brunzema$*$, Paul and {Von Rohr*}, Alexander and Trimpe, Sebastian},
  booktitle   = {Conference on Decision and Control (CDC)},
  abstract    = {Changing conditions or environments can cause system dynamics to vary over time. To ensure optimal control performance, controllers should adapt to these changes. When the underlying cause and time of change is unknown, we need to rely on online data for this adaptation. In this paper, we will use time-varying Bayesian optimization (TVBO) to tune controllers online in changing environments using appropriate prior knowledge on the control objective and its changes. Two properties are characteristic of many online controller tuning problems: First, they exhibit incremental and lasting changes in the objective due to changes to the system dynamics, e.g., through wear and tear. Second, the optimization problem is convex in the tuning parameters. Current TVBO methods do not explicitly account for these properties, resulting in poor tuning performance and many unstable controllers through over-exploration of the parameter space. We propose a novel TVBO forgetting strategy using Uncertainty-Injection (UI), which incorporates the assumption of incremental and lasting changes. The control objective is modeled as a spatio-temporal Gaussian process (GP) with UI through a Wiener process in the temporal domain. Further, we explicitly model the convexity assumptions in the spatial dimension through GP models with linear inequality constraints. In numerical experiments, we show that our model outperforms the state-of-the-art method in TVBO, exhibiting reduced regret and fewer unstable parameter configurations. },
  html        = {https://ieeexplore.ieee.org/document/9992649},
  arxiv       = {2207.11120},
  bibtex_show = {true},
  year        = {2022},
  preview     = {UITVBO_paper_image.png}
}

@article{brunzema2025event,
  title       = {Event-Triggered Time-Varying {Bayesian} Optimization},
  author      = {Brunzema, Paul and {von Rohr}, Alexander and Solowjow, Friedrich and Trimpe, Sebastian},
  journal     = {Transactions on Machine Learning Research (TMLR)},
  abstract    = {We consider the problem of sequentially optimizing a time-varying objective function using time-varying Bayesian optimization (TVBO). Current approaches to TVBO require prior knowledge of a constant rate of change to cope with stale data arising from time variations. However, in practice, the rate of change is usually unknown. We propose an event-triggered algorithm, ET-GP-UCB, that treats the optimization problem as static until it detects changes in the objective function and then resets the dataset. This allows the algorithm to adapt online to realized temporal changes without the need for exact prior knowledge. The event trigger is based on probabilistic uniform error bounds used in Gaussian process regression. We derive regret bounds for adaptive resets without exact prior knowledge of the temporal changes and show in numerical experiments that ET-GP-UCB outperforms competing GP-UCB algorithms on both synthetic and real-world data. The results demonstrate that ET-GP-UCB is readily applicable without extensive hyperparameter tuning.},
  arxiv       = {2208.10790},
  html        = {https://openreview.net/forum?id=WEYMCLu8u7},
  pdf         = {https://openreview.net/pdf?id=WEYMCLu8u7},
  selected    = {true},
  bibtex_show = {true},
  year        = {2025},
  preview     = {ETGPUCB.png}
}

@article{hose2024fine,
  author      = {Henrik Hose and Paul Brunzema and Alexander {von Rohr} and Alexander Gr\"afe and Angela P. Schoellig and Sebastian Trimpe},
  title       = {Fine-Tuning of Neural Network Approximate {MPC} without Retraining via {Bayesian} Optimization},
  journal     = {International Conference on Robot Intelligence Technologies and Applications (RiTA)},
  abstract    = {Approximate model-predictive control (AMPC) aims to imitate an MPCs behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical due to the need to repeatedly generate a new dataset and retrain the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC’s optimization problem. However, currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver of a cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.},
  html        = {https://arxiv.org/abs/2512.14350},
  pdf         = {https://arxiv.org/pdf/2512.14350},
  arxiv       = {2512.14350},
  award       = {The paper was awarded the **Best Student Paper Award**.},
  award_name  = {Best Student Paper Award},
  bibtex_show = {true},
  year        = {2025},
  preview     = {wb-bo.png}
}

@article{brunzema2026vision,
  title={Vision-Conditioned Variational {Bayesian} Last Layer Dynamics Models},
  author={Brunzema, Paul and Lew, Thomas and Zhang, Ray and Shirasawa, Takeru and Subosits, John and Greiff, Marcus},
  journal={arXiv preprint arXiv:2601.09178},
  year={2026},
  abstract={Agile control of robotic systems often requires anticipating how the environment affects system behavior. For example, a driver must perceive the road ahead to anticipate available friction and plan actions accordingly. Achieving such proactive adaptation within autonomous frameworks remains a challenge, particularly under rapidly changing conditions. Traditional modeling approaches often struggle to capture abrupt variations in system behavior, while adaptive methods are inherently reactive and may adapt too late to ensure safety. We propose a vision-conditioned variational Bayesian last-layer dynamics model that leverages visual context to anticipate changes in the environment. The model first learns nominal vehicle dynamics and is then fine-tuned with feature-wise affine transformations of latent features, enabling context-aware dynamics prediction. The resulting model is integrated into an optimal controller for vehicle racing. We validate our method on a Lexus LC500 racing through water puddles. With visionconditioning, the system completed all 12 attempted laps under varying conditions. In contrast, all baselines without visual context consistently lost control, demonstrating the importance of proactive dynamics adaptation in high-performance applications.},
  preview     = {TRI_paper_image.jpeg},
  bibtex_show = {true},
    selected    = {true},
    html        = {https://arxiv.org/abs/2601.09178},
  pdf         = {https://arxiv.org/pdf/2601.09178},
}

@inproceedings{brunzema2024variational,
  title       = {Variational Last Layers for {Bayesian} Optimization},
  author      = {Brunzema, Paul and Jordahn, Mikkel and Willes, John and Trimpe, Sebastian and Snoek, Jasper and Harrison, James},
  booktitle   = {NeurIPS Workshop on Bayesian Decision-making and Uncertainty},
  abstract    = {Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured, such as those defined by Euclidean metrics. However, the performance of GPs depends on the choice of kernel, and kernel selection for complex correlation structures is often difficult. While Bayesian neural networks are a promising direction for higher capacity surrogate models, they have so far seen limited use due to a combination of cost of use and poor performance. In this paper, we explore the potential of neural networks with variational Bayesian last layers (VBLLs), which offer a simple and computationally lightweight approach to Bayesian uncertainty quantification in neural networks. Our findings suggest that VBLL networks significantly outperform GPs and other BNN architectures on tasks with complicated input correlations, and match the performance of well-tuned GPs on established benchmark tasks. These results highlight their promise as an alternative surrogate model for BO.},
  html        = {https://openreview.net/forum?id=lsFa23pHCH},
  pdf         = {https://openreview.net/pdf?id=lsFa23pHCH},
  bibtex_show = {true},
  year        = {2024},
  preview     = {VBLL-BO_paper_image.png}
}

@article{hose2026mini,
  title={The {Mini Wheelbot} Dataset: High-Fidelity Data for Robot Learning},
  author={Hose, Henrik and Brunzema, Paul and Subhasish, Devdutt and Trimpe, Sebastian},
  journal={arXiv preprint arXiv:2601.11394},
  abstract={The development of robust learning-based control algorithms for unstable systems requires high-quality, realworld data, yet access to specialized robotic hardware remains a significant barrier for many researchers. This paper introduces a comprehensive dynamics dataset for the Mini Wheelbot, an open-source, quasi-symmetric balancing reaction wheel unicycle. The dataset provides 1 kHz synchronized data encompassing all onboard sensor readings, state estimates, groundtruth poses from a motion capture system, and third-person video logs. To ensure data diversity, we include experiments across multiple hardware instances and surfaces using various control paradigms, including pseudo-random binary excitation, nonlinear model predictive control, and reinforcement learning agents. We include several example applications in dynamics model learning, state estimation, and time-series classification to illustrate common robotics algorithms that can be benchmarked on our dataset.},
  year={2026},
  pdf={https://arxiv.org/pdf/2601.11394},
  html={https://arxiv.org/abs/2601.11394},
  arxiv={2601.11394},
  preview     = {wb_dataset.png},
  bibtex_show = {true}
}

@inproceedings{brunzema2024bayesian,
  title       = {Bayesian Optimization via Continual Variational Last Layer Training},
  author      = {Brunzema, Paul and Jordahn, Mikkel and Willes, John and Trimpe, Sebastian and Snoek, Jasper and Harrison, James},
  booktitle   = {International Conference on Learning Representations (ICLR)},
  award       = {The paper was selected for a **Spotlight** presentation at the conference.},
  award_name  = {Spotlight},
  abstract    = {Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured (such as those defined by Euclidean metrics) and their ability to be efficiently updated online. However, the performance of GPs depends on the choice of kernel, and kernel selection for complex correlation structures is often difficult or must be made bespoke. While Bayesian neural networks (BNNs) are a promising direction for higher capacity surrogate models, they have so far seen limited use due to poor performance on some problem types. In this paper, we propose an approach which shows competitive performance on many problem types, including some that BNNs typically struggle with. We build on variational Bayesian last layers (VBLLs), and connect training of these models to exact conditioning in GPs. We exploit this connection to develop an efficient online training algorithm that interleaves conditioning and optimization. Our findings suggest that VBLL networks significantly outperform GPs and other BNN architectures on tasks with complex input correlations, and match the performance of well-tuned GPs on established benchmark tasks.},
  html        = {https://openreview.net/forum?id=1jcnvghayD},
  pdf         = {https://openreview.net/pdf?id=1jcnvghayD},
  arxiv       = {2412.09477},
  selected    = {true},
  bibtex_show = {true},
  year        = {2025},
  preview     = {VBLL-BO_paper_image.png}
}

@inproceedings{brunzema2025bayesqp,
  title       = {{BayeSQP}: Bayesian Optimization through Sequential Quadratic Programming},
  author      = {Brunzema, Paul and Trimpe, Sebastian},
  booktitle   = {Advances in Neural Information Processing Systems (NeurIPS)},
  award       = {The paper was selected for a **Spotlight** presentation at the conference.},
  award_name  = {Spotlight},
  abstract    = {We introduce BayeSQP, a novel algorithm for general black-box optimization that merges the structure of sequential quadratic programming with concepts from Bayesian optimization. BayeSQP employs second-order Gaussian process surrogates for both the objective and constraints to jointly model the function values, gradients, and Hessian from only zero-order information. At each iteration, a local subproblem is constructed using the GP posterior estimates and solved to obtain a search direction. Crucially, the formulation of the subproblem explicitly incorporates uncertainty in both the function and derivative estimates, resulting in a tractable second-order cone program for high probability improvements under model uncertainty. A subsequent one-dimensional line search via constrained Thompson sampling selects the next evaluation point. Empirical results show that BayeSQP outperforms state-of-the-art methods in specific high-dimensional settings. Our algorithm offers a principled and flexible framework that bridges classical optimization techniques with modern approaches to black-box optimization.},
  html        = {https://openreview.net/forum?id=FEugj28qhC},
  pdf         = {https://openreview.net/pdf?id=FEugj28qhC},
  selected    = {true},
  arxiv       = {2602.03232},
  bibtex_show = {true},
  year        = {2025},
  preview     = {BayeSQP_paper_image.png}
}


@inproceedings{holzapfel2024event,
  title       = {Event-Triggered Safe {Bayesian} Optimization on Quadcopters},
  author      = {Holzapfel*, Antonia and Brunzema*, Paul and Trimpe, Sebastian},
  booktitle   = {Learning for Dynamics and Control Conference (L4DC)},
  html        = {https://proceedings.mlr.press/v242/holzapfel24a},
  pdf         = {https://proceedings.mlr.press/v242/holzapfel24a/holzapfel24a.pdf},
  arxiv       = {2312.08058},
  abstract    = {Bayesian optimization (BO) has proven to be a powerful tool for automatically tuning control parameters without requiring knowledge of the underlying system dynamics. Safe BO methods, in addition, guarantee safety during the optimization process, assuming that the underlying objective function does not change. However, in real-world scenarios, time-variations frequently occur, for example, due to wear in the system or changes in operation. Utilizing standard safe BO strategies that do not address time-variations can result in failure as previous safe decisions may become unsafe over time, which we demonstrate herein. To address this, we introduce a new algorithm, Event-Triggered SafeOpt (ETSO), which adapts to changes online solely relying on the observed costs. At its core, ETSO uses an event trigger to detect significant deviations between observations and the current surrogate of the objective function. When such change is detected, the algorithm reverts to a safe backup controller, and exploration is restarted. In this way, safety is recovered and maintained across changes. We evaluate ETSO on quadcopter controller tuning, both in simulation and hardware experiments. ETSO outperforms state-of-the-art safe BO, achieving superior control performance over time while maintaining safety.},
  bibtex_show = {true},
  year        = {2024},
  preview     = {ETSO.png}
}

@inproceedings{brunzema2024neural,
  title       = {Neural Processes with Event Triggers for Fast Adaptation},
  author      = {Brunzema*, Paul and Kruse*, Paul and Trimpe, Sebastian},
  booktitle   = {Learning for Dynamics and Control Conference (L4DC)},
  html        = {https://proceedings.mlr.press/v242/brunzema24a},
  pdf         = {https://proceedings.mlr.press/v242/brunzema24a/brunzema24a.pdf},
  abstract    = {Traditionally, first-principle models are used to monitor and control dynamical systems. However, modeling complex systems using first principles can be challenging. Learning the dynamics from data using neural networks has emerged as a viable alternative. In practice, some parameters of a system may vary across different system instances, but training separate neural networks for all possible parameter combinations can be infeasible. Therefore, meta-learning using, e.g., conditional neural processes (CNPs), aims to learn a prior model over the system dynamics for various parameters. These models can then adapt on deployment to the parameters of a system instance using a context set composed of past observations. However, changes in parameters can also occur online during operation and naively adding past observations across parameter variations to the context set can distort the model’s latent representation, leading to inaccurate predictions over time. This paper introduces an adaptation scheme to enable CNPs to cope with such online variations. We combine a sliding window to accommodate gradual variations with the use of event triggers to detect sudden changes. The event triggers are based on concentration inequalities, they reset the context set of the CNP once observations deviate significantly from the CNP’s predictions. We validate our concepts on two nonlinear dynamical systems under parameter variations and demonstrate that our approaches decrease the prediction error over time as well as their efficacy for control.},
  bibtex_show = {true},
  year        = {2024},
  preview     = {cnps.png}
}

@inproceedings{harrison2024heteroscedastic,
  title       = {Heteroscedastic Variation Bayesian Last Layers},
  author      = {James Harrison and John Willes and Paul Brunzema and Jasper Snoek},
  booktitle   = {Advances on Approximate Bayesian Inference (AABI) Workshop},
  html        = {https://proceedings.mlr.press/v242/brunzema24a},
  pdf         = {https://openreview.net/pdf?id=I7PeIAwJ7I},
  abstract    = {We improve the performance of Variational Bayesian Last Layer (VBLL) networks by better modeling aleatoric noise. In particular, we (1) Introduce t-VBLL layers, which perform variational inference for the noise covariance, and (2) Introduce Het-VBLL, a Bayesian last layer scheme to model heteroscedastic noise. These methods are based on novel, analytically tractable evidence lower bounds. We show that these novel design elements extend the capabilities of VBLLs at minimal additional cost, and substantially improve performance.},
  bibtex_show = {true},
  year        = {2025},
  preview     = {AABI.png}
}